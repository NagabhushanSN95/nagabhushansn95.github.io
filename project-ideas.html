<!--Shree KRISHNAya Namaha-->
<!DOCTYPE html>
<html lang="en">
<head>
    <title>Nagabhushan S N - Project Ideas</title>
    <link rel="stylesheet" href="res/styles/w3.css">
    <link rel="stylesheet" href="res/styles/common.css">
    <link rel="stylesheet" href="res/styles/navigation.css">
<!--    <script src='https://kit.fontawesome.com/a076d05399.js'></script>-->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <script src="res/scripts/navigation.js"></script>

    <link rel="stylesheet" href="project-ideas/res/styles/project-ideas.css">
    <!--    <script src="project-ideas/res/scripts/project-ideas.js"></script>-->

    <meta charset="UTF-8">
    <meta name="description" content="Project Ideas Page">
    <meta name="author" content="Nagabhushan S N">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
</head>
<body>

<!-- Navigation Sidebar - Start-->
<nav class="w3-sidebar w3-collapse w3-light-grey w3-animate-left" style="z-index:3;width:260px" id="nav_panel">
    <div class="w3-container">
        <i onclick="w3_close()" class="w3-hide-large w3-right w3-xlarge w3-padding w3-hover-grey" title="close menu">
            <i class="fas fa-times"></i>
        </i>
        <h4><b>Nagabhushan S N</b></h4>
    </div>
    <div class="w3-bar-block">
        <a href="index.html" class="w3-bar-item w3-button">
            <i class="fa fa-home" style="width: 25px"></i> Home
        </a>
        <a href="research.html" class="w3-bar-item w3-button">
            <i class="fa fa-flask" style="width: 25px"></i> Research
        </a>
<!--        <a href="publications.html" class="w3-bar-item w3-button">-->
<!--            <i class="fa fa-file-alt" style="width: 25px"></i> Publications-->
<!--        </a>-->
        <a href="publications.html" class="w3-bar-item w3-button">
            <img class="fa" src="res/images/file-alt-solid.svg" style="width: 13px; margin-right: 12px" alt="fa-file-alt"/> Publications
        </a>
<!--        <a href="projects.html" class="w3-bar-item w3-button">-->
<!--            <i class="fa fa-project-diagram" style="width: 25px"></i> Projects-->
<!--        </a>-->
        <a href="projects.html" class="w3-bar-item w3-button">
            <img class="fa" src="res/images/diagram-project-solid.svg" style="width: 15px; margin-right: 10px" alt="fa-project-diagram"/> Projects
        </a>
        <a href="work-experience.html" class="w3-bar-item w3-button">
            <i class="fa fa-briefcase" style="width: 25px"></i> Work Experience
        </a>
        <a href="miscellaneous.html" class="w3-bar-item w3-button">
            <i class="fa fa-ellipsis-h" style="width: 25px"></i> Miscellaneous
        </a>
        <a href="project-ideas.html" class="w3-bar-item w3-button selected-page">
            <i class="fa fa-lightbulb-o" style="width: 25px"></i> Project Ideas
        </a>
        <a href="res/documents/SNB_CV.pdf" target="_blank" class="w3-bar-item w3-button" title="Download CV">
            Curriculum Vitae (CV)
        </a>
        <a href="res/documents/SNB_Resume.pdf" target="_blank" class="w3-bar-item w3-button" title="Download Resume">
            Resume
        </a>
    </div>
    <div class="w3-panel w3-large">
        <a href="https://linkedin.com/in/nagabhushan-s-n-52391a68" target="_blank"><i
                class="fa fa-linkedin-square w3-hover-opacity" style="color: #0072b1"></i></a> &ensp;
        <a href="https://github.com/NagabhushanSN95" target="_blank"><i
                class="fa fa-github w3-hover-opacity"></i></a> &ensp;
        <a href="https://stackexchange.com/users/4060371/nagabhushan-s-n?tab=accounts" target="_blank"><img
                class="fa w3-hover-opacity" src="res/images/Icon_StackExchange.svg" alt="Stack Exchange"
                style="width: 25px"/></a> &ensp;
        <a href="https://scholar.google.com/citations?hl=en&user=kFbFZHIAAAAJ" target="_blank"><i
                class="ai ai-google-scholar color-google-scholar w3-hover-opacity"></i></a> &ensp;
        <a href="https://orcid.org/0000-0002-2266-759X" target="_blank"><i
                class="ai ai-orcid color-orcid w3-hover-opacity"></i></a>
    </div>
    <div class="w3-panel w3-large">
        <a href="https://publons.com/researcher/3835716/nagabhushan-s-n/" target="_blank"><i
                class="ai ai-publons color-publons w3-hover-opacity"></i></a> &ensp;
        <a href="https://www.semanticscholar.org/author/Nagabhushan%60-S-N/1666222692" target="_blank"><i
                class="ai ai-semantic-scholar w3-hover-opacity"></i></a> &ensp;
        <a href="https://dblp.org/pid/264/5930" target="_blank"><i
                class="ai ai-dblp w3-hover-opacity"></i></a> &ensp;
    </div>
</nav>

<!-- Overlay effect when opening sidebar on small screens -->
<div class="w3-overlay w3-hide-large w3-animate-opacity" onclick="w3_close()" style="cursor:pointer"
     title="close side menu" id="content_overlay"></div>
<!-- Navigation Sidebar - End-->

<!-- Page content - Start -->
<div class="w3-main" style="margin-left:260px">
    <span class="w3-button w3-hide-large w3-xxlarge w3-hover-text-grey" onclick="w3_open()"><i
            class="fa fa-bars"></i></span>
    <div class="w3-container">
        <h1 class="page-title">Project Ideas</h1>
    </div>

    <div class="w3-container" style="width: 80%; margin-left: 10%; padding-top: 50px">
        <p>
            The below are a list of ideas that I think are interesting. Some of them may be used for academic course
            projects and some may be insufficient. You can build on such ideas. If you have any queries regarding these
            ideas, please don't hesitate to contact me. If you do try these ideas, kindly update me, and I'll update
            your findings here (with due credits, of course). If there is already an existing work on the ideas listed
            below, kindly update me with that as well. I'll add a link to the relevant content.
        </p>
    </div>

    <!-- TODO: Add Contents (Research, Product)    -->

    <div class="w3-container" id="research-ideas" style="width: 80%; margin-left: 10%; padding-top: 30px">
        <a href="project-ideas.html#research-ideas"><h3><u>Research Based Ideas</u></h3></a>

        <div class="w3-container" id="2024-02-12">
            <a href="project-ideas.html#2024-02-12"><h5>2024-02-12: Is learnt depth prior needed in SCADE (sparse-input NeRF)</h5></a>
            <p>
                SCADE [2] was proposed to train NeRF [1] with few inputs.
                Specifically, a mono-depth model is used to obtain depth prior.
                Instead of obtaining a single depth prior, SCADE uses a mono-depth model that takes a latent vector to condition the uncertainty in predicting mono-depth.
                By providing multiple latent vectors sampled from a unit Gaussian, multiple depth priors are obtained per pixel thereby obtaining a distribution on depth.
                The depth distribution from NeRF is obtained using coarse NeRF and samples are obtained by inverse transform sampling.
                Space carving loss is imposed between the samples from the depth distributions of prior and NeRF.
            </p>
            <p>
                Is it necessary to obtain depth prior from a deep neural network? Deep neural networks are known to suffer from generalization issues.
                Instead, can we obtain depth prior from Plane-Sweep-Volumes?
                Specifically, based on the errors/variance at every plane, apply inverse transform sampling to obtain depth prior samples.
                How well would this do across different datasets?
            </p>

            [1] Ben Mildenhall <i>et al.</i>
            <a href="https://dl.acm.org/doi/abs/10.1145/3503250"
               target="_blank">"NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis"</a>, ECCV 2020. <br>
            [2] Mikaela Angelina Uy <i>et al.</i>
            <a href="https://openaccess.thecvf.com/content/CVPR2023/html/Uy_SCADE_NeRFs_from_Space_Carving_With_Ambiguity-Aware_Depth_Estimates_CVPR_2023_paper.html"
               target="_blank">"SCADE: NeRFs from Space Carving With Ambiguity-Aware Depth Estimates"</a>, CVPR 2023.

            <hr class="w3-dark-gray" style="height: 1px; width: 100%">
        </div>

        <div class="w3-container" id="2023-10-13">
            <a href="project-ideas.html#2023-10-13"><h5>2023-10-13: Analyzing Mip-NeRF Components</h5></a>
            <p>
                Mip-NeRF [2] was proposed to address antialiasing limitation of NeRF [1].
                Specifically, if the same scene-region/object is captured at different scales, NeRF breaks down and introduces blur or aliasing artifacts.
                Mip-NeRF addresses this by shooting a cone into the scene instead of a single ray and sampling conical frustums instead of individual 3D points.
                An ideal solution to obtain the density/color of thw conical frustum would involve sampling multiple points in the conical frustum and averaging them.
                To make the model computationally efficient, Mip-NeRF averages the input (positional encoded 3D points) instead of the output; thereby querying the MLP only once per frustum.
                While the idea is reasonable, the end result of integrated positional encoded features is that the encoded features of a point varies depending on its distance to the camera.
                So, what would happen if one simply provides the distance to the camera as another input to the NeRF and use original positional encoding instead of integrated positional encoding? Would this perform as well as Mip-NeRF?
            </p>

            [1] Ben Mildenhall <i>et al.</i>
            <a href="https://dl.acm.org/doi/abs/10.1145/3503250"
               target="_blank">"NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis"</a>, ECCV 2020. <br>
            [2] Jonathan T Barron <i>et al.</i>
            <a href="https://openaccess.thecvf.com/content/ICCV2021/html/Barron_Mip-NeRF_A_Multiscale_Representation_for_Anti-Aliasing_Neural_Radiance_Fields_ICCV_2021_paper.html"
               target="_blank">"Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields"</a>, ICCV 2021.

            <hr class="w3-dark-gray" style="height: 1px; width: 100%">
        </div>

        <div class="w3-container" id="2023-10-12">
            <a href="project-ideas.html#2023-10-12"><h5>2023-10-12: Stress Testing Mip-NeRF</h5></a>
            <p>
                Mip-NeRF [2] was proposed to address antialiasing limitation of NeRF [1].
                Specifically, if the same scene-region/object is captured at different scales, NeRF breaks down and introduces blur or aliasing artifacts.
                Mip-NeRF addresses this by providing expected positional encoding of all the 3D points in a conical frustum along the cone as input to the MLP, instead of positional encoding of the centroid of the cone as done in NeRF.
                For evaluation, the train and test sets of the original NeRF are downsampled to three different scales. On the multi-scale dataset, Mip-NeRF performs significantly better than NeRF.
            </p>
            <p>
                Thinking from fundamental graphics principles, antialiasing should be implemented by integrating the sigma/radiance (output of MLP) over the conical frustum.
                Integrating the inputs may not have the desired antialiasing effect.
                That is, the input to the MLP is different at different scales and the MLP can exploit this to hack through and provide antialiased outputs.
                Thus, if the scale changes at test time/novel view to a value unseen during training, Mip-NeRF could potentially output junk.
                However, it is not clear if this will actually happen.
                The MLP could learn to integrate the sigma/radiance when the inputs corresponding to higher dimension are zero.
                So, it is interesting to experiment this: train Mip-NeRF on the original Blender dataset [1] and test it on the multi-scale Blender dataset [2].
            </p>

            [1] Ben Mildenhall <i>et al.</i>
            <a href="https://dl.acm.org/doi/abs/10.1145/3503250"
               target="_blank">"NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis"</a>, ECCV 2020. <br>
            [2] Jonathan T Barron <i>et al.</i>
            <a href="https://openaccess.thecvf.com/content/ICCV2021/html/Barron_Mip-NeRF_A_Multiscale_Representation_for_Anti-Aliasing_Neural_Radiance_Fields_ICCV_2021_paper.html"
               target="_blank">"Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields"</a>, ICCV 2021.

            <hr class="w3-dark-gray" style="height: 1px; width: 100%">
        </div>

        <div class="w3-container" id="2023-07-20">
            <a href="project-ideas.html#2023-07-20"><h5>2023-07-20: Is pixelNeRF feature encoder secretly a mono depth estimator</h5></a>
            <p>
                Vanilla NeRF [1] needs to be trained for every scene, which was overcome by pixelNeRF [2], which is a generalized NeRF model.
                The core idea is to employ a feature extractor on the input images and condition the NeRF MLP on the feature at the corresponding 3D point in addition to the 3D point location and viewing direction.
                The feature at the queried 3D point is obtained by projecting the 3D point onto the image plane and bilinearly interpolating the feature map at the projected point.
                As a result, all the 3D points along the ray corresponding to a pixel will share the same feature. How can the NeRF MLP distinguish the 3D points? (See "<a href="#2023-07-15">What is the role of MLP in generalizable NeRF models?</a>" to understand why).
                Is it that the feature encoder embeds mono-depth information into the feature vector and the MLP checks if the depth of the 3D point matches with the depth embedded in the feature vector to determine the volume density?
            </p>

            <p>
                One way to validate this is to add a few more conv layers on top of pixelNeRF encoder and train the additional layers to predict depth from the features.
                Note that the pixelNeRF feature encoder should be frozen during this training.
                True depth or depth estimated by a state-of-the-art mono-depth model can be used to supervise the training of the additional layers.
                If this model is able to estimate depth to high accuracy, it indicates that the pixelNeRF feature encoder is acting also as a mono-depth estimator and embedding the depth information in the feature vectors.
            </p>

            [1] Ben Mildenhall <i>et al.</i>
            <a href="https://dl.acm.org/doi/abs/10.1145/3503250"
               target="_blank">"NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis"</a>, ECCV 2020. <br>
            [2] Alex Yu <i>et al.</i>
            <a href="https://openaccess.thecvf.com/content/CVPR2021/html/Yu_pixelNeRF_Neural_Radiance_Fields_From_One_or_Few_Images_CVPR_2021_paper.html"
               target="_blank">"pixelNeRF: Neural Radiance Fields From One or Few Images"</a>, CVPR 2021.

            <hr class="w3-dark-gray" style="height: 1px; width: 100%">
        </div>

        <div class="w3-container" id="2023-07-15">
            <a href="project-ideas.html#2023-07-15"><h5>2023-07-15: What is the role of MLP in generalizable NeRF models?</h5></a>
            <p>
                Vanilla NeRF [1] needs to be trained for every scene, which was overcome by generalized NeRF models such as pixelNeRF [2], MVS-NeRF [3] and so on.
                The core idea is to employ a feature extractor on the input images and condition the NeRF MLP on the feature at the corresponding 3D point in addition to the 3D point location and viewing direction.
                When using in the generalized setup without any scene-specific fine-tuning, this raises the question that whether the 3D point location contributes any information.
                One way to validate this is to remove the 3D location from the inputs and feed only the feature at the 3D point and the viewing direction to the NeRF MLP and analyze the performance.
                If there is no significant drop in generalized (unseen scenes) performance, then it can be concluded that the MLP is simply acting as a decoder for the 3D features.
                How does the performance vary for scene-specific fine-tuning?
            </p>

            [1] Ben Mildenhall <i>et al.</i>
            <a href="https://dl.acm.org/doi/abs/10.1145/3503250"
               target="_blank">"NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis"</a>, ECCV 2020. <br>
            [2] Alex Yu <i>et al.</i>
            <a href="https://openaccess.thecvf.com/content/CVPR2021/html/Yu_pixelNeRF_Neural_Radiance_Fields_From_One_or_Few_Images_CVPR_2021_paper.html"
               target="_blank">"pixelNeRF: Neural Radiance Fields From One or Few Images"</a>, CVPR 2021. <br>
            [3] Anpei Chen <i>et al.</i>
            <a href="https://openaccess.thecvf.com/content/ICCV2021/html/Chen_MVSNeRF_Fast_Generalizable_Radiance_Field_Reconstruction_From_Multi-View_Stereo_ICCV_2021_paper.html"
               target="_blank">"MVSNeRF: Fast Generalizable Radiance Field Reconstruction From Multi-View Stereo"</a>, ICCV 2021.

            <hr class="w3-dark-gray" style="height: 1px; width: 100%">
        </div>

        <div class="w3-container" id="2020-05-03">
            <a href="project-ideas.html#2020-05-03"><h5>2020-05-03: Adversarial training and bias of deep features
                towards local texture</h5></a>
            <p>
                Although deep networks have become extremely powerful for object recognition, they perform poorly in the
                presence of adversarial attacks. In [1], the authors propose a method to train deep networks to be
                robust to adversarial attacks by enforcing BPFC regularization. In [2], the authors show that deep
                networks trained on ImageNet-1k database are biased towards local texture and hence when tested on edge
                maps (where no local texture is present), their efficiency reduces.
            </p>
            <p>
                Since adversarial attacks/examples mainly modify local properties of images, it would be interesting to
                see if training a deep network to be robust to adversarial attacks would lead to lower bias towards
                local texture. One way to verify this would be to test the performance of deep networks, which are
                trained to be robust to adversarial attacks, on edge maps of images from ImageNet-1k database or any
                other similar database.
            </p>

            [1] Sravanti Addepalli <i>et al.</i>
            <a href="https://openaccess.thecvf.com/content_CVPR_2020/html/Addepalli_Towards_Achieving_Adversarial_Robustness_by_Enforcing_Feature_Consistency_Across_Bit_CVPR_2020_paper.html"
               target="_blank">"Towards Achieving Adversarial Robustness by Enforcing Feature Consistency Across Bit
                Planes"</a>, CVPR 2020. <br>
            [2] Robert Geirhos <i>et al.</i>
            <a href="https://openreview.net/forum?id=Bygh9j09KX&noteId=HJxSI4x527"
               target="_blank">"ImageNet-trained CNNs are biased towards texture; increasing shape bias improves
                accuracy and robustness"</a>, ICLR 2019.

            <hr class="w3-dark-gray" style="height: 1px; width: 100%">
        </div>

        <div class="w3-container" id="2020-03-25">
            <a href="project-ideas.html#2020-03-25"><h5>2020-03-25: Does ResNet features capture global shape?</h5></a>
            <p>
                In [1], the authors show that deep networks trained on ImageNet-1k database are biased towards local
                texture than global shape. One of the experiments conducted in the paper is to get ResNet-50 [2]
                predictions on edge map of an image. In the paper it is shown that accuracy of ResNet reduces on edge
                maps of images.
            </p>
            <p>
                It would be interesting to see if ResNet features (features tapped before the global pooling operation)
                contain information about global shape. One way to check this is to freeze the weights of previous
                layers of ResNet-50 and train only the last softmax layer on edge maps of images in ImageNet-1k
                database. And then test this new model with edge maps of images.
            </p>

            [1] Robert Geirhos <i>et al.</i>
            <a href="https://openreview.net/forum?id=Bygh9j09KX&noteId=HJxSI4x527"
               target="_blank">"ImageNet-trained CNNs are biased towards texture; increasing shape bias improves
                accuracy and robustness"</a>, ICLR 2019. <br>
            [2] Kaiming He <i>et al.</i>
            <a href="https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html"
               target="_blank">"Deep Residual Learning for Image Recognition"</a>, CVPR 2016.

            <hr class="w3-dark-gray" style="height: 1px; width: 100%">
        </div>

        <div class="w3-container" id="2019-11-19">
            <a href="project-ideas.html#2019-11-19"><h5>2019-11-19: How well can MoCoGAN decompose video into Motion and
                Content?</h5></a>
            <p>
                A video generative model has been proposed in [1], which decomposes video into content and motion.
                Content latent vector remains same across all frames while motion latent vector is generated using a
                recurrent network. A decoder then generates a frame using content and motion latent vectors.
            </p>
            <p>
                It would be interesting to check how good the decomposition is working. To check this, one can change
                the content latent vector mid-generation. Ideally the motion should remain the same while the person
                should change.
            </p>

            [1] Sergey Tulyakov <i>et al.</i>
            <a href="https://openaccess.thecvf.com/content_cvpr_2018/html/Tulyakov_MoCoGAN_Decomposing_Motion_CVPR_2018_paper.html"
               target="_blank"> "MoCoGAN: Decomposing Motion and Content for Video Generation"</a>, CVPR 2018.
            <hr class="w3-dark-gray" style="height: 1px; width: 100%">
        </div>

        <div class="w3-container" id="2019-09-23">
            <a href="project-ideas.html#2019-09-23"><h5>2019-09-23: Quantitative Evaluation of GANs</h5></a>
            <p>
                Quantitatively evaluating GAN models has been found challenging. Here, I propose a simple idea to
                evaluate them based on how we would evaluate linear regression. Given a trained GAN model and an image
                from test set, backpropagate the gradients (by keeping the generator weights fixed) to find the input
                which can generate an image close to the test image. The error between the generated image and the test
                image, averaged over all images in the test set, may be evaluated as a quantitative measure for
                evaluating GANs. Various error/similarity measures like MSE, SSIM, VGG MSE or VGG cosine similarity can
                be experimented with.
            </p>
            <!--            <hr class="w3-dark-gray" style="height: 1px; width: 100%">-->
        </div>

    </div>
    <hr class="w3-dark-gray" style="height: 1px; width: 80%; margin-left: 10%">

    <div class="w3-container" id="product-ideas" style="width: 80%; margin-left: 10%; padding-top: 30px">
        <a href="project-ideas.html#product-ideas"><h3><u>Product Based Ideas</u></h3></a>

        <div class="w3-container" id="2018-09-28">
            <a href="project-ideas.html#2018-09-28"><h5>2018-09-28: Image Retrieval using Face Recognition</h5></a>
            <p>
                Build a face detection and recognition system, which can be applied on any new directory of images.
                Build a GUI such that whenever a model encounters an unknown face, it requests the user to enter the
                name of the person. The face recognition model should be able to dynamically learn to classify new
                faces. The model builds an index of the people appearing in the photos and stores it. Later, when a
                query for a particular person is issued, the model should retrieve all the images containing that
                person.
            </p>
            <h6 style="margin-bottom: 0"><b>Update:</b></h6>
            <p style="margin-top: 0">
                This idea has already been implemented in Google Photos. Nonetheless, we may not be willing to upload
                all our private photos to Google Photos. In that case, having an offline model, specifically trained on
                people in our friend circle, may be useful. Although this model may not be able to perform as good as
                Google Photos.
            </p>
            <!--            <hr class="w3-dark-gray" style="height: 1px; width: 100%">-->
        </div>

    </div>
    <hr class="w3-dark-gray" style="height: 1px; width: 80%; margin-left: 10%">
</div>
<!-- Page content - End -->

</body>
</html>
